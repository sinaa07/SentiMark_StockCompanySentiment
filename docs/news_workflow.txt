Let’s keep it simple and incremental. Below is a clean flow that inserts a GPT/Gemini “formatting” layer before FinBERT, and removes content filtering entirely, while fitting your current files.

### High-level Flow (v2)
1. `news_collector.py`: Collect raw articles as-is (no filtering).
2. NEW LLM layer: Normalize/condense each article into a consistent schema (title + canonical_text + metadata), ready for FinBERT.
3. `finbert_preprocessor.py`: Consume the LLM-normalized text directly; ensure idempotent text cleaning only.
4. `finbert_client.py`: Run sentiment on the normalized text.
5. `db_manager.py`: Persist results and cache LLM outputs to avoid re-calls.
6. `pipeline.py`: Orchestrate steps; remove content filtering stage.

### What to change in existing files
- `src/pipeline.py`
  - Remove any references to `content_filter.py` / `content_filter_v0.py`.
  - Insert a new step after collection to call the LLM formatter (batch if possible).
  - Ensure downstream stages (preprocessor → finbert → db) use the normalized payload.
- `src/news_collector.py`
  - Keep behavior to fetch and dedupe articles.
  - Ensure each article has stable identifiers (e.g., `url`, `source`, `published_at`, `tickers`) for caching LLM results.
- `src/finbert_preprocessor.py`
  - Accept already-normalized text from the LLM layer.
  - Minimize transformations: light cleaning only (whitespace, strange unicode); no filtering.
  - Enforce length constraints FinBERT expects (truncate safely if needed).
- `src/finbert_client.py`
  - No change in API, but ensure it reads the normalized text field (e.g., `canonical_text`) consistently.
- `src/db_manager.py`
  - Add tables/columns for LLM-normalized outputs and their checksum/hash for caching.
  - Upsert semantics keyed by article URL or a content hash to prevent double-charging LLM.
- `app.py`
  - If the UI shows raw vs processed, optionally surface the normalized text as an inspectable field.
- `config/settings.py` and `config/api_keys.py`
  - Add config for model provider (openai vs gemini), model name, temperature, max tokens, cost caps, and retry settings.

### Files to remove
- `src/content_filter.py`
- `src/content_filter_v0.py`
- Any pipeline references to these.

### New files to add and their responsibilities
- `src/llm_formatter.py`
  - Purpose: Given a raw article dict, call GPT/Gemini to produce a normalized, FinBERT-ready payload.
  - Core features:
    - Provider-agnostic interface with a simple `format_article(article) -> normalized_article`.
    - Prompt templates that enforce a strict output schema and style:
      - Required fields: `title`, `canonical_text`, `summary` (optional), `source`, `published_at`, `tickers`, `language`, `url`, `llm_version`, `llm_raw_tokens`.
      - Constraints: English (or original language passthrough + translated flag), remove boilerplate, exclude disclaimers/market open/close noise, keep factual tone, avoid hallucinating tickers.
    - Batch mode: `format_articles([...])` to reduce overhead where feasible.
    - Caching hooks: check db/file cache before calling; write results after success.
    - Provider adapters for OpenAI and Gemini.
    - Rate-limit and retry/backoff with budget guardrails.
- `src/llm_providers.py`
  - Purpose: Thin adapters for OpenAI/Gemini calls.
  - Core features:
    - `OpenAIProvider.call(prompt, params) -> text/json`
    - `GeminiProvider.call(prompt, params) -> text/json`
    - Standardize errors, timeouts, and token usage returns.
- `src/prompts/news_normalization_prompt.txt` (or `.md`)
  - Purpose: Single source of truth for the normalization instructions.
  - Core features:
    - Clear schema spec; JSON output only; no prose.
    - Style constraints and banned content notes (no opinions, no price targets unless present).
    - Shortness policy: 1-2 sentence summary, canonical body under a token budget.
- `src/schema.py`
  - Purpose: Pydantic dataclasses (or lightweight validators) to validate LLM outputs.
  - Core features:
    - `NormalizedArticle` with strict types and length bounds.
    - Validation for date formats, ticker arrays, non-empty `canonical_text`.
- `src/cache.py` (optional if you centralize in `db_manager.py`)
  - Purpose: Compute content hash (title + url + raw_text) and manage LLM response cache.
  - Core features:
    - `get_cached_normalization(hash)`, `set_cached_normalization(hash, result)`

### Data Contract between stages
- Input to LLM formatter (from `news_collector.py`):
  - `url`, `title`, `raw_text`, `source`, `published_at`, `tickers` (from mapping), `language` (if detectable)
- Output from LLM formatter (consumed by `finbert_preprocessor.py`):
  - `title` (cleaned, factual)
  - `canonical_text` (concise, noise-free body for sentiment)
  - `summary` (1-2 sentence optional)
  - `tickers` (validated against `data/stock_mappings.json`)
  - `source`, `published_at`, `url`
  - `language`, `translated` (bool if translation used)
  - `llm_version`, `tokens_used`, `cache_key`

### Minimal changes to tests
- `tests/test_news_scraper.py`
  - Keep as-is for collection.
- Add tests:
  - `tests/test_llm_formatter.py`: prompt conformance, schema validation, caching behavior, fallback on malformed JSON.
  - `tests/test_pipeline_integration.py`: ensure pipeline skips filtering, calls formatter, feeds FinBERT.
- Update:
  - `tests/test_sentiment.py`: expect `canonical_text` as the sentiment input.
  - Remove filtering-related tests.

### Migration and caching
- DB changes:
  - Add `normalized_articles` table or extend existing with fields:
    - `cache_key` (unique), `normalized_json`, `tokens_used`, `provider`, `model`, `created_at`, `updated_at`
  - Add an index on `cache_key`.
- Caching policy:
  - Compute `cache_key = sha256(url + title + first_1k_chars_raw_text)`.
  - On hit, skip LLM call; on miss, call and store.
- Backfill:
  - Optional utility in `pipeline.py` to re-normalize recent N days with a budget cap.

### Cost/budget guardrails
- Hard cap on total tokens per run in `settings.py`.
- Per-article truncation before LLM (e.g., keep first N chars) to bound spend.
- Batch scheduling and exponential backoff on rate limits.

### Rollout plan
- Phase 1: Add new modules (`llm_formatter.py`, `llm_providers.py`, `schema.py`, prompts), wire into `pipeline.py`, keep FinBERT and DB unchanged except for new table/columns.
- Phase 2: Remove `content_filter*` and their imports from code and tests.
- Phase 3: Add caching and budget guardrails; monitor logs.
- Phase 4: (Optional) UI tweak in `app.py` to display normalized summary.

This gives you a straightforward “LLM normalization → FinBERT” pipeline, simplifies preprocessing, removes content filtering, and controls cost with caching and constraints.